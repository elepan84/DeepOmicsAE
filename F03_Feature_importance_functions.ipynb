{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ecdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc72a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_latent(encoder, data, eps):\n",
    "\n",
    "\"\"\"\n",
    "This function computes the importance scores for the features of a dataset by perturbing each feature and observing the changes in the latent space of an encoder model.\n",
    "\n",
    "Parameters:\n",
    "encoder (model): The encoder model used to project data into latent space.\n",
    "data (DataFrame): The input data whose features' importance are to be determined.\n",
    "eps (float): The perturbation amount added to each feature in the data.\n",
    "\n",
    "Output:\n",
    "importance_scores (2D numpy array): An array with importance scores for each feature across all \n",
    "dimensions of the latent space.\n",
    "\"\"\"\n",
    "\n",
    "    data = data.values\n",
    "    original_latent = encoder.predict(data)[0]\n",
    "    importance_scores = np.zeros((data.shape[1], original_latent.shape[1]))\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        perturbed_data = data.copy()\n",
    "        perturbed_data[:, i] += eps\n",
    "        perturbed_latent = encoder.predict(perturbed_data)[0]\n",
    "        \n",
    "        # Measure change in latent space\n",
    "        change = np.mean(np.abs(original_latent - perturbed_latent), axis=0)\n",
    "        \n",
    "        importance_scores[i, :] = change\n",
    "\n",
    "    return importance_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995e0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_percentile_indices(scores):\n",
    "\n",
    "\"\"\"\n",
    "This function calculates the 70th percentile value (threshold) for each column in a 2D array (scores)\n",
    "and returns indices of elements that are greater or equal to this threshold. The output is a list \n",
    "of these indices for each column (top_indices), along with the corresponding threshold values (thresholds).\n",
    "\"\"\"\n",
    "    num_latent_units = scores.shape[1]\n",
    "    top_indices = []\n",
    "    thresholds = []\n",
    "    \n",
    "    for i in range(num_latent_units):\n",
    "        threshold = np.percentile(scores[:, i], 70) # 70th percentile value\n",
    "        indices = np.where(scores[:, i] >= threshold)[0] # indices where score > threshold\n",
    "        top_indices.append(indices)\n",
    "        thresholds.append(threshold)\n",
    "    return top_indices, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcb7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(dfs, thresholds):\n",
    "    \"\"\"\n",
    "    Function to plot density plots for multiple dataframes in a 2x2 grid and save to a PDF file.\n",
    "    \n",
    "    Parameters:\n",
    "    dfs (list): A list of dataframes.\n",
    "    thresholds (list): A list of threshold values.\n",
    "    \"\"\"\n",
    "    # Ensure dfs and thresholds have the same length\n",
    "    assert len(dfs) == len(thresholds), \"The lengths of dfs and thresholds must match.\"\n",
    "\n",
    "    # Create a figure with 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8.5/2, 11/2))  # Letter size paper in inches\n",
    "\n",
    "    # Flatten axs to make it easier to iterate over\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Calculate global min and max\n",
    "    global_min = min(df['Scaled Importance Values'].min() for df in dfs)\n",
    "    global_75th_percentile = max(df['Scaled Importance Values'].quantile(0.99) for df in dfs)\n",
    "    #print(global_75th_percentile)\n",
    "    \n",
    "    # Create handles and labels for the legend\n",
    "    handles, labels = None, None\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        # Plot density plot for red-coded values\n",
    "        sns.kdeplot(data=df[df['color'] == 'red'], x='Scaled Importance Values', fill=True, color='red', label='Important features', ax=ax)\n",
    "\n",
    "        # Plot density plot for blue-coded values\n",
    "        sns.kdeplot(data=df[df['color'] == 'blue'], x='Scaled Importance Values', fill=True, color='blue', label='Other features', ax=ax)\n",
    "\n",
    "        # Ignore warnings from scikit-learn\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "        # Add a vertical line for the threshold\n",
    "        ax.axvline(scaler.transform([[thresholds[i]]])[0,0], color='gray', linestyle='--')\n",
    "\n",
    "        # Set plot title and labels\n",
    "        ax.set_title(f'Module {i+1}', fontsize=12)\n",
    "        ax.set_xlabel('Importance Values', fontsize=12)\n",
    "        ax.set_ylabel('', fontsize=12)\n",
    "        \n",
    "        # Set x-axis limits to global min and max\n",
    "        ax.set_xlim(global_min, global_75th_percentile)\n",
    "\n",
    "        # If this is the first plot, save the handles and labels for the legend\n",
    "        if handles is None and labels is None:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5, bottom=0.2)\n",
    "    \n",
    "    # Add a single legend for the whole figure\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2, bbox_to_anchor=(0.5, 0))\n",
    "    \n",
    "    # Define the PDF output file\n",
    "    pdf_pages = PdfPages('distribution_important_feature_scores.pdf')\n",
    "\n",
    "    # Save the figure to the PDF file\n",
    "    pdf_pages.savefig(fig, bbox_inches='tight')  # Use bbox_inches='tight' to include the legend in the saved figure\n",
    "\n",
    "    # Close the PdfPages object\n",
    "    pdf_pages.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a set of dataframes with the important values for each neuron in the latent layer\n",
    "### normalize the importance values in the range between 0 to 1\n",
    "### this data will be used to plot the distribution of importance scores\n",
    "\n",
    "def importance_scores_scaling(importance_scores, thresholds):\n",
    "    \"\"\"\n",
    "    This function scales the importance scores for each feature across all dimensions of latent space, \n",
    "    using Min-Max normalization.\n",
    "\n",
    "    Parameters:\n",
    "    importance_scores (2D numpy array): An array with unscaled importance scores.\n",
    "    thresholds (list): A list of threshold values. Each threshold corresponds to a \n",
    "    column in 'importance_scores'.\n",
    "    A score greater than its corresponding threshold gets labeled 'red', otherwise 'blue'.\n",
    "\n",
    "\n",
    "    Output:\n",
    "    dfs (list of DataFrames): A list of DataFrames where each DataFrame corresponds to a dimension \n",
    "    in the latent space and contains scaled importance scores for each feature.\n",
    "\n",
    "    Note: The function also adds a color label ('red' or 'blue') for each score based on \n",
    "    the threshold values provided as input.\n",
    "    \"\"\"\n",
    "     \n",
    "    i = importance_scores.shape[1]\n",
    "\n",
    "    # Create empty DataFrames for each latent unit\n",
    "    dfs = []\n",
    "    for i in range(importance_scores.shape[1]):\n",
    "        df = pd.DataFrame({\n",
    "            'x': [i+1] * importance_scores.shape[0],\n",
    "            'Importance Values': importance_scores[:, i]\n",
    "        })\n",
    "         # Determine colors based on threshold for each DataFrame\n",
    "        threshold = thresholds[i]\n",
    "        df['color'] = ['red' if value > threshold else 'blue' for value in df['Importance Values']]\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Access individual DataFrames (e.g., df1, df2, df3, df4)\n",
    "    df1, df2, df3, df4 = dfs\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to 'Importance Values' in the combined dataframe\n",
    "    scaler.fit(combined_df[['Importance Values']])\n",
    "\n",
    "    # Apply transformation to each individual dataframe in dfs\n",
    "    for df in dfs:\n",
    "        df['Scaled Importance Values'] = scaler.transform(df[['Importance Values']])\n",
    "    \n",
    "    return(dfs, scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_xgboost_env",
   "language": "python",
   "name": "tensorflow_xgboost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
