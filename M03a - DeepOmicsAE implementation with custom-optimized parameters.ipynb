{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from F03_Feature_importance_functions import *\n",
    "from F02_autoencoder_model_optimization_final_new import *\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from upsetplot import UpSet\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets using pandas\n",
    "df = pd.read_csv(\"M01_output_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af53b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_X_prot = slice(col_start, col_stop)\n",
    "cols_X_met = slice(col_start2, col_stop2)\n",
    "cols_clin = slice(col_start3, col_stop3)\n",
    "cols_X_expr = slice(col_start4, col_stop4)\n",
    "y_label = 'y_column_name'\n",
    "n_all_feat = cols_X_met.stop\n",
    "n_all_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35761fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the data set to assess the degree of separation between clusters on the initial data\n",
    "X = df.iloc[:, 0:n_all_feat]\n",
    "y = df[y_label]\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "# Calculate Silhouette Score\n",
    "pca_silhouette_score_initial = silhouette_score(X_pca, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transfer parameters for feature selection and extraction obtained with the model optimization algorithm\n",
    "### (Method 2, M02)\n",
    "\n",
    "%store -r kprot kmet latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Feature selection #########################################################\n",
    "X_new_prot, df_selected_prot = feature_selection(df, kprot, cols_X_prot, y_label) ## select proteomic features\n",
    "X_new_met, df_selected_met = feature_selection(df, kmet, cols_X_met, y_label) ## select metabolomic features\n",
    "\n",
    "## assemble a dataframe containing only the selected features\n",
    "df_selected = pd.concat([df_selected_prot, \n",
    "                         df_selected_met, df.iloc[:, cols_clin], df[y_label]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Feature extraction #########################################################\n",
    "\n",
    "\n",
    "# Define the autoencoder model architecture\n",
    "n_feat = df_selected.iloc[:, :-1].shape[1]\n",
    "n2 = round(n_feat/1.5)\n",
    "if (n2 <= 70):\n",
    "    n3 = round(n2/2)\n",
    "if (n2 > 70) and (n2 <= 200):\n",
    "    n3 = round(n2/5)\n",
    "else:\n",
    "    n3 = round(n2/6)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_selected.iloc[:, :-1]\n",
    "y = df_selected.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "### run the autoencoder first with n_epochs set to 300, then use min_val_loss_epochs for the final run\n",
    "## verbose can be either set to 1 to visualize the losses as the model runs, or 0 to silence them\n",
    "\n",
    "history, val_loss, min_val_loss_epoch, bottleneck_features, encoder = ae_model_setup_and_run(n_feat, n2, n3, latent, X, X_train, X_val, 300, verbose = 0)\n",
    "history, val_loss, min_val_loss_epoch, bottleneck_features, encoder = ae_model_setup_and_run(n_feat, n2, n3, latent, X, X_train, X_val, min_val_loss_epoch, verbose = 0)                                                                                       \n",
    "\n",
    "# Create a pandas DataFrame for the extracted bottleneck features\n",
    "extracted_features_df = pd.DataFrame(bottleneck_features, columns=[f\"Feature_{i}\" for i in range(latent)])\n",
    "extracted_features_df[y_label] = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the extracted features data set to assess the degree of separation between clusters \n",
    "## after feature extraction\n",
    "\n",
    "# Separate features and labels\n",
    "X = bottleneck_features\n",
    "\n",
    "# Perform PCA and t-SNE\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_extr = pca.fit_transform(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "pca_silhouette_score_extracted_feat = silhouette_score(X_pca_extr, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the PCA of the initial data set and of the extracted feature\n",
    "## data set to visualize and compare the degree of separation\n",
    "\n",
    "\n",
    "# This line sets the default global font size\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Create a gridspec to partition our figure\n",
    "gs = GridSpec(1, 2, width_ratios=[4, 0.8]) \n",
    "\n",
    "fig = plt.figure(figsize=(6.5, 4.5)) # Adjust as necessary\n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "\n",
    "# Plot PCA embeddings\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "\n",
    "# Adding the legend in the space of the second subplot\n",
    "ax_legend = fig.add_subplot(gs[1])\n",
    "ax_legend.axis('off')\n",
    "scatter_legend = ax_legend.legend(*scatter.legend_elements(), title='Classes', loc='center')\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA on initial data')\n",
    "\n",
    "plt.savefig(\"PCA_initial_data.pdf\", format='pdf', dpi=300) # This will save the plot as a PDF\n",
    "print(f'Silhouette Score PCA on initial data: {pca_silhouette_score_initial}')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6.5, 4.5)) # Adjust as necessary\n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "\n",
    "scatter = ax.scatter(X_pca_extr[:, 0], X_pca_extr[:, 1], c=y, cmap='viridis')\n",
    "# Adding the legend in the space of the second subplot\n",
    "ax_legend = fig.add_subplot(gs[1])\n",
    "ax_legend.axis('off')\n",
    "scatter_legend = ax_legend.legend(*scatter.legend_elements(), title='Classes', loc='center')\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA on extracted features')\n",
    "plt.savefig(\"PCA_extracted_features.pdf\", format='pdf', dpi=300) # This will save the plot as a PDF\n",
    "\n",
    "print(f'Silhouette Score PCA on extracted features: {pca_silhouette_score_extracted_feat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d068bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute importance scores for the original features relative to the latent layer\n",
    "\n",
    "X = df_selected.iloc[:, :-1]\n",
    "\n",
    "importance_scores = feature_importance_latent(encoder, X, eps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the indices of the important features in the original dataframe\n",
    "### Also, calculate the threshold values to define important features\n",
    "## the threshold values are calculated as the 70th percentile of the set of importance values\n",
    "# for each neuron in the latent layer\n",
    "\n",
    "top_indices, thresholds = top_percentile_indices(importance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ab0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a set of dataframes with the important values for each neuron in the latent layer\n",
    "### normalize the importance values in the range between 0 to 1\n",
    "### this data will be used to plot the distribution of importance scores\n",
    "\n",
    "dfs, scaler = importance_scores_scaling(importance_scores, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate density plots of the importance scores \n",
    "plot_density(dfs, thresholds, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5127fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create dataframes with the selected \n",
    "## important features for each neuron in the latent layer\n",
    "# top_indices is a list of lists. \n",
    "# Each sublist is a list of top indices for each neuron in the latent layer.\n",
    "\n",
    "df_selected_list = []\n",
    "\n",
    "for indices in top_indices:\n",
    "    df_selected_list.append(df_selected.iloc[:, indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save list of important features to txt files that will be used for the MetaboAnalyst analysis\n",
    "## (Method 4)\n",
    "for i, df in enumerate(df_selected_list):\n",
    "    with open(f\"module_{i+1}.txt\", \"w\") as file:\n",
    "        for col in df.columns:\n",
    "            file.write(col + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_xgboost_env",
   "language": "python",
   "name": "tensorflow_xgboost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
